<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction">
  <meta name="keywords" content="Action Segmentation, Timestamp Supervision, Human Object Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/saif-iftekar-sayed">Saif Sayed, </a></span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/rezaghoddoosian">Reza Ghoddoosian, </a></span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/bhaskar-c-trivedi/">Bhaskar Trivedi, </a></span>
            <span class="author-block">
              <a href="https://athitsos.utasites.cloud">Vassilis Athitsos</a></sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Texas at Arlington &nbsp; </span>

          </div>

          <h1 style="font-size:24px;font-weight:bold">CVPR 2023</h1>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/papers/Sayed_A_New_Dataset_and_Approach_for_Timestamp_Supervised_Action_Segmentation_CVPRW_2023_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Supp Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/supplemental/Sayed_A_New_Dataset_CVPRW_2023_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supp</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1k10xURqylCnFMexyg-GDV9OcXwIx7soz/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a href="https://github.com/saifsayed/rec-dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/saifsayed/rec-dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered"> 
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper focuses on leveraging Human Object Interaction (HOI) information to improve temporal action segmentation under timestamp supervision, where only one frame is annotated for each action segment. This information is obtained from an off-the-shelf pre-trained HOI detector, that requires no additional HOI-related annotations in our experimental datasets. Our approach generates pseudo labels by expanding the annotated timestamps into intervals and allows the system to exploit the spatio-temporal con- tinuity of human interaction with an object to segment the video. We also propose the (3+1)Real-time Cooking (ReC)1 dataset as a realistic collection of videos from 30 participants cooking 15 breakfast items. Our dataset has three main properties: 1) to our knowledge, the first to offer syn- chronized third and first person videos, 2) it incorporates diverse actions and tasks, and 3) it consists of high resolution frames to detect fine-grained information. In our experi- ments we benchmark state-of-the-art segmentation methods under different levels of supervision on our dataset. We also quantitatively show the advantages of using HOI information, as our framework improves its baseline segmentation method on several challenging datasets with varying viewpoints, providing improvements of up to 10.9% and 5.3% in F1 score and frame-wise accuracy respectively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<br>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img style='height: auto; width: 100%; object-fit: contain' src="static/images/system.png" alt="system_image">
      <h2 class="subtitle has-text-centered">
        The proposed training framework. The secondary labels generator creates new pseudo ground-truth, κ using the HOI detections ρ and existing timestamp annotations. The binarized pseudo ground-truth(α) also provides new supervisory signal to the primary label generator for generating frame-wise labels β. 
      </h2>
    </div>
  </div>
</section>

<br>
<br>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">(3+1) Real-time Cooking (ReC) Dataset</h2>
    <div class="hero-body">
      <img style='height: auto; width: 100%; object-fit: contain' src="static/images/Dataset_properties.png" alt="data_props">
      <h2 class="subtitle has-text-centered">
        Real-time instructional video dataset comparison. * indicates approximation due to a hidden test set. “Views” refers to 3rd person. Some statistical discrepancies between 1ReC and 3ReC is due to frame loss in some videos. “Envir.” includes various camera setups.
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Video Samples</h2>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls width="100%"><source src="static/videos/Avocado_toast_diverse.m4v" type="video/mp4"></video>
      <h2 class="subtitle has-text-centered">
        Diverse environments for Dish "Avocado Toast"
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls width="100%"><source src="static/videos/orange_juice_sample.m4v" type="video/mp4"></video>
      <h2 class="subtitle has-text-centered">
        Multiple Viewpoints for the same Dish "Orange Juice"
      </h2>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Qualitative Results</h2>
    <div class="hero-body">
      <img style='height: auto; width: 100%; object-fit: contain' src="static/images/qualitative_results.png" alt="qual_res">
      <h2 class="subtitle has-text-centered">
        Qualitative results on (a) 50Salads, (b) GTEA datasets and (c)(3+1)ReC. The baseline method suffers from over- segmentation, while our approach alleviates this issue by utilizing the continuity in human object interaction. 
      </h2>
    </div>
  </div>
</section>

<br>
  <br>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{sayed2023new,
        title={A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction},
        author={Sayed, Saif and Ghoddoosian, Reza and Trivedi, Bhaskar and Athitsos, Vassilis},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={3132--3141},
        year={2023}
      } 
    </code></pre>
  </div>
</section>



<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">3D-moments</a>.
        </div>
      </div>
    </div>
</footer>


</body>
</html>
