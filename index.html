<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction">
  <meta name="keywords" content="Action Segmentation, Timestamp Supervision, Human Object Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~qqw/">Saif Sayed, </a></span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~zl548/">Reza Ghoddoosian, </a></span>
            <span class="author-block">
              <a href="http://salesin.cs.washington.edu/">Bhaskar Trivedi, </a></span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~snavely/">Vassilis Athitsos</a></sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Texas at Arlington &nbsp; </span>

          </div>

          <h1 style="font-size:24px;font-weight:bold">CVPR 2023</h1>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/papers/Sayed_A_New_Dataset_and_Approach_for_Timestamp_Supervised_Action_Segmentation_CVPRW_2023_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Supp Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/supplemental/Sayed_A_New_Dataset_CVPRW_2023_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supp</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1k10xURqylCnFMexyg-GDV9OcXwIx7soz/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a href="https://github.com/saifsayed/rec-dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/saifsayed/rec-dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <div align="center" style="margin-top:80px;" style="margin-bottom:120px;">\
<img style='height: auto; width: 50%; object-fit: contain' src="static/images/system.png" alt="system_image">
The proposed training framework. The secondary labels generator creates new pseudo ground-truth, κ using the HOI detections ρ and existing timestamp annotations. The binarized pseudo ground-truth(α) also provides new supervisory signal to the primary label generator for generating frame-wise labels β. 
</div>  -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img style='height: auto; width: 75%; object-fit: contain' src="static/images/system.png" alt="system_image">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> The proposed training framework. The secondary labels generator creates new pseudo ground-truth, κ using the HOI detections ρ and existing timestamp annotations. The binarized pseudo ground-truth(α) also provides new supervisory signal to the primary label generator for generating frame-wise labels β. 
      </h2>
    </div>
  </div>
</section>


<!-- <div align="center" style="margin-top:80px;" style="margin-bottom:120px;">
  <img style='height: auto; width: 50%; object-fit: contain' src="static/images/Dataset_properties.png" alt="system_image">
  Real-time instructional video dataset comparison. * indicates approximation due to a hidden test set. “Views” refers to 3rd person. Some statistical discrepancies between 1ReC and 3ReC is due to frame loss in some videos. “Envir.” includes various camera setups.
</div>  -->


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls width="100%">
        <source src="static/videos/teaser2.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">3D Moments</span> turns a duplicate photo pair into space-time videos
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/zoom-in_3988-0-1.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/zoom-in_000357-0-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/zoom-in_000301-0-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay playsinline controls muted loop width="100%">
            <source src="static/videos/000654_up-down-0-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay playsinline controls muted loop width="100%">
            <source src="static/videos/000204_side-0-1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  
  <!-- <div class="container is-max-desktop"> -->
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper focuses on leveraging Human Object Interaction (HOI) information to improve temporal action segmen- tation under timestamp supervision, where only one frame is annotated for each action segment. This information is obtained from an off-the-shelf pre-trained HOI detector, that requires no additional HOI-related annotations in our experimental datasets. Our approach generates pseudo la- bels by expanding the annotated timestamps into intervals and allows the system to exploit the spatio-temporal con- tinuity of human interaction with an object to segment the video. We also propose the (3+1)Real-time Cooking (ReC)1 dataset as a realistic collection of videos from 30 partic- ipants cooking 15 breakfast items. Our dataset has three main properties: 1) to our knowledge, the first to offer syn- chronized third and first person videos, 2) it incorporates di- verse actions and tasks, and 3) it consists of high resolution frames to detect fine-grained information. In our experi- ments we benchmark state-of-the-art segmentation methods under different levels of supervision on our dataset. We also quantitatively show the advantages of using HOI information, as our framework improves its baseline segmentation method on several challenging datasets with varying view- points, providing improvements of up to 10.9% and 5.3% in F1 score and frame-wise accuracy respectively.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Abstract. -->


    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <a id="overview_video"></a>
          <video id="teaser" playsinline controls width="100%">
            <source src="static/videos/overview_video.mp4" type="video/mp4">
          </video>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>

</section>

 <br>
  <br>

<section>
  <div class="columns is-centered has-text-centered">
    <h2 class="title is-3"> More Results</h2>
</div>
<br>

<div id="wrapper" style="text-align:center;"> 
    <video autoplay controls muted loop playsinline id="result_video_side" height="400px" width="auto" > 
        <source type="video/mp4" src="static/videos/2558_side-0-1.mp4" /> 
    </video>

    <video autoplay controls muted loop playsinline id="result_video_side"> 
        <source type="video/mp4" src="static/videos/000654_side-0-1.mp4" /> 
    </video>
    <video  autoplay controls muted loop playsinline id="result_video_side"> 
        <source type="video/mp4" src="static/videos/000242_side-0-1.mp4" /> 
    </video>

    <div class="clear"></div>
Tracking camera motion (horizontal) 
</div>

<br>

<div id="wrapper" style="text-align:center;  padding: 2em 0;"> 
    <video  autoplay controls muted loop playsinline id="result_video_updown"> 
        <source type="video/mp4" src="static/videos/000679_up-down-0-1.mp4" /> 
    </video>

    <video autoplay controls muted loop playsinline id="result_video_updown"> 
        <source type="video/mp4" src="static/videos/5428_up-down-0-1.mp4" /> 
    </video>

    <video  autoplay controls muted loop playsinline id="result_video_updown"> 
        <source type="video/mp4" src="static/videos/000350_up-down-0-1.mp4" /> 
    </video>

    <div class="clear"></div> 

Tracking camera motion (vertical) 
</div>

<br>

<div id="wrapper" style="text-align:center;"> 
    <video  autoplay controls muted loop playsinline id="result_video_zooming"> 
        <source type="video/mp4" src="static/videos/zoom-in_000204-0-1.mp4" /> 
    </video>

    <video autoplay controls muted loop playsinline id="result_video_zooming"> 
        <source type="video/mp4" src="static/videos/zoom-in_0161-0-1.mp4" /> 
    </video>

    <video  autoplay controls muted loop playsinline id="result_video_zooming"> 
        <source type="video/mp4" src="static/videos/zoom-in_000304-0-1.mp4" /> 
    </video>

    <div class="clear"></div> 

Zooming-In camera motion 
</div>
</section>

<br>
<br>

<section>
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3"> Failure Cases</h2>
    </div>
<br>
    <div id="wrapper" style="text-align:center;"> 
    <video  autoplay controls muted loop playsinline id="failure_video"> 
        <source type="video/mp4" src="static/videos/failure1.mp4" /> 
    </video>
&nbsp;&nbsp;
    <video  autoplay controls muted loop playsinline id="failure_video"> 
        <source type="video/mp4" src="static/videos/failure2.mp4" /> 
    </video>

    <div class="clear"></div> 
Our method cannot handle motions that are too large/challenging, and doesn't work well with incorrectly estimated monocular depths.

</div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{sayed2023new,
        title={A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction},
        author={Sayed, Saif and Ghoddoosian, Reza and Trivedi, Bhaskar and Athitsos, Vassilis},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={3132--3141},
        year={2023}
      }
    </code></pre>
  </div>
</section>



<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">3D-moments</a>.
        </div>
      </div>
    </div>
</footer>


</body>
</html>
